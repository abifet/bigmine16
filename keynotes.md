---
layout: page
title: Keynotes
image:
  feature: sf.png
---


## Joseph Bradley

**Title:** Foundations for Scaling ML in Apache Spark

<a href=""><img src="images/JBradley.jpg" alt="Joseph Bradley" height="80" width="80" class="" /></a>

Joseph Bradley is a Software Engineer and Apache Spark PMC member working on machine learning and graph processing at Databricks. Previously, he was a postdoc at UC Berkeley after receiving his Ph.D. in Machine Learning from Carnegie Mellon U. in 2013. His research included probabilistic graphical models, parallel sparse regression, and aggregation mechanisms for peer grading in MOOCs.

**Abstract:** Apache Spark has become the most active open source Big Data project, and its Machine Learning library MLlib has seen rapid growth in usage.  A critical aspect of MLlib and Spark is the ability to scale: the same code used on a laptop can scale to 100’s or 1000’s of machines.  This talk will describe ongoing and future efforts to make MLlib even faster and more scalable by integrating with two key initiatives in Spark.  The first is Catalyst, the query optimizer underlying DataFrames and Datasets.  The second is Tungsten, the project for approaching bare-metal speeds in Spark via memory management, cache-awareness, and code generation.  This talk will discuss the goals, the challenges, and the benefits for MLlib users and developers.  More generally, we will reflect on the importance of integrating ML with the many other aspects of big data analysis.

About MLlib: MLlib is a general Machine Learning library providing many ML algorithms, feature transformers, and tools for model tuning and building workflows.  The library benefits from integration with the rest of Apache Spark (SQL, streaming, Graph, core), which facilitates ETL, streaming, and deployment.  It is used in both ad hoc analysis and production deployments throughout academia and industry.


## Inside the Atoms: Mining a Network of Networks and Beyond.

**Abstract** Networks (i.e., graphs) appears in many high-impact applications. Often these networks are collected from different sources, at different times, at different granularities. In this talk, I will present our recent work on mining such multiple networks. First, we will present two models - one on modeling a set of inter-connected networks (NoN); and the other on modeling a set of inter-connected co-evolving time series (NoT). For both models, we will show that by treating networks as context, we are able to model more complicate real-world applications. Second, we will present some algorithmic examples on how to do mining with such new models, including ranking, imputation and prediction. Finally, we will demonstrate the effectiveness of our new models and algorithms in some applications, including bioinformatics, and sensor networks.


**Bio** 

<a href=""><img src="images/hh.png" alt="Hanghang Tong" height="80" width="80" class="" /></a>

Hanghang Tong is currently an assistant professor at School of Computing, Informatics, and Decision Systems Engineering (CIDSE), Arizona State University since August 2014.  Before that, he was an assistant professor at Computer Science Department, City College, City University of New York, a research staff member at IBM T.J. Watson Research Center and a Post-doctoral fellow in Carnegie Mellon University. He received his M.Sc and Ph.D. degree from Carnegie Mellon University in 2008 and 2009, both majored in Machine Learning. His research interest is in large scale data mining for graphs and multimedia. He has received several awards, including one ‘test of time’ award (ICDM 10-Year highest impact paper award), four best paper awards and four ‘best of conference’. He has published over 100 referred articles and more than 20 patents.

